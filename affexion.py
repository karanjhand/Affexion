# -*- coding: utf-8 -*-
"""Affexion

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vAsgh88j5AYVEcphsldozdz5TNsn_5CW
"""

!pip install librosa scikit-learn matplotlib seaborn

from google.colab import drive
drive.mount('/content/drive')

import os
import librosa
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import KFold
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from collections import Counter
from sklearn.utils.class_weight import compute_class_weight

# Device handling
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# Emotion labels (match folder names)
EMOTIONS = ['Boredom', 'Panic', 'Excitement', 'Uncertainity']

EMOTION_TO_IDX = {emo: i for i, emo in enumerate(EMOTIONS)}

# Google Drive Paths for training dataset
TRAIN_DIR = "/content/drive/MyDrive/EmotionData/train/"
#TRAIN_DIR = "/content/drive/MyDrive/CMPT_419_2025/EmotionData/train/"


# Audio settings
SAMPLE_RATE = 16000
MAX_LEN = 1000  # For padding MFCC sequences (around 10 seconds of audio)

def extract_mfcc_sequence(file_path, max_len=MAX_LEN):
    y, sr = librosa.load(file_path, sr=SAMPLE_RATE)
    # Extract MFCC and its first and second derivatives
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=25)
    delta = librosa.feature.delta(mfcc)
    delta2 = librosa.feature.delta(mfcc, order=2)

    features = np.vstack([mfcc, delta, delta2])  # (75, T)

    # Normalize each feature
    features = (features - np.mean(features, axis=1, keepdims=True)) / (np.std(features, axis=1, keepdims=True) + 1e-8)

    # Pad or truncate
    if features.shape[1] < max_len:
        pad = max_len - features.shape[1]
        features = np.pad(features, ((0, 0), (0, pad)), mode='constant')
    else:
        features = features[:, :max_len]

    return features.T  # Did transpose (T, 75)

class EmotionSequenceDataset(Dataset):
    def __init__(self, base_dir, print_logs=False):
        self.data = []
        self.labels = []
        for emotion in EMOTIONS:
            folder = os.path.join(base_dir, emotion.capitalize())
            for filename in os.listdir(folder):
                if filename.endswith(".wav"):
                    path = os.path.join(folder, filename)
                    features = extract_mfcc_sequence(path)
                    label = EMOTION_TO_IDX[emotion]
                    self.data.append((features, label))
                    self.labels.append(label)
                    if print_logs:
                        print(f"Loaded: {filename} â†’ {emotion}")

        if print_logs:
            counts = Counter(self.labels)
            print("\nClass distribution:")
            for k, v in counts.items():
                print(f"{EMOTIONS[k]}: {v} samples")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        features, label = self.data[idx]
        return torch.tensor(features, dtype=torch.float32), label

# Multi-scale CNN block to capture features at different time resolutions
class MultiScaleCNN(nn.Module):
    def __init__(self, input_dim=75, out_channels=64):
        super(MultiScaleCNN, self).__init__()
        self.conv3 = nn.Conv1d(input_dim, out_channels, kernel_size=3, padding=1)
        self.conv5 = nn.Conv1d(input_dim, out_channels, kernel_size=5, padding=2)
        self.conv7 = nn.Conv1d(input_dim, out_channels, kernel_size=7, padding=3)
        self.bn = nn.BatchNorm1d(out_channels * 3)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2)

    def forward(self, x):
        # x shape: (batch, channels, time)
        x3 = self.relu(self.conv3(x))
        x5 = self.relu(self.conv5(x))
        x7 = self.relu(self.conv7(x))
        # Concatenate along the channel dimension
        x_cat = torch.cat([x3, x5, x7], dim=1)
        x_cat = self.bn(x_cat)
        x_cat = self.dropout(x_cat)
        return x_cat  # shape: (batch, out_channels*3, time)

# Simple self-attention mechanism to focus on important temporal features
class SelfAttention(nn.Module):
    def __init__(self, input_dim):
        super(SelfAttention, self).__init__()
        self.query = nn.Linear(input_dim, input_dim)
        self.key   = nn.Linear(input_dim, input_dim)
        self.value = nn.Linear(input_dim, input_dim)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x):
        # x shape: (batch, seq, input_dim)
        Q = self.query(x)  # (batch, seq, input_dim)
        K = self.key(x)    # (batch, seq, input_dim)
        V = self.value(x)  # (batch, seq, input_dim)
        # Compute attention scores
        scores = torch.bmm(Q, K.transpose(1, 2)) / np.sqrt(x.size(-1))
        attn_weights = self.softmax(scores)
        attn_output = torch.bmm(attn_weights, V)
        # Residual connection adds the original input back in
        return attn_output + x

# Enhanced model inspired by wav2vec: multi-scale CNN -> BiLSTM -> Self-Attention -> FC layers
class EnhancedEmotionModel(nn.Module):
    def __init__(self, input_dim=75, hidden_dim=128, num_classes=len(EMOTIONS)):
        super(EnhancedEmotionModel, self).__init__()
        self.multi_cnn = MultiScaleCNN(input_dim=input_dim, out_channels=64)  # Output channels: 64*3 = 192
        self.lstm = nn.LSTM(192, hidden_dim, num_layers=2, batch_first=True, bidirectional=True)
        self.attention = SelfAttention(hidden_dim * 2)  # Input dimension: hidden_dim * 2 (bidirectional)
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim * 2, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, num_classes)
        )

    def forward(self, x):
        # x shape: (batch, time, features)
        x = x.permute(0, 2, 1)  # shape becomes (batch, features, time)
        x = self.multi_cnn(x)   # (batch, 192, time)
        x = x.permute(0, 2, 1)   # (batch, time, 192)
        lstm_out, _ = self.lstm(x)  # (batch, time, hidden_dim*2)
        attn_out = self.attention(lstm_out)  # (batch, time, hidden_dim*2)
        pooled = torch.mean(attn_out, dim=1)  # Global average pooling over time
        return self.fc(pooled)

#Load Entire Dataset for Training Only
dataset = EmotionSequenceDataset(TRAIN_DIR)

# Compute class weights
labels = [label for _, label in dataset]

train_loader = DataLoader(dataset, batch_size=8, shuffle=True)

#Training Loop
#Taken from Code provided by Professor in Assignments
model = EnhancedEmotionModel().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)
num_epoch = 15

for epoch in range(num_epoch):
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    for features, labels in train_loader:
        features = features.to(device)
        labels = torch.tensor(labels).to(device)

        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        preds = torch.argmax(outputs, dim=1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    scheduler.step()
    acc = correct / total
    print(f"Epoch {epoch+1} | Loss: {total_loss:.4f} | Training Accuracy: {acc:.3f}")

# Validation on External Dataset (val/)
VAL_DIR = "/content/drive/MyDrive/EmotionData/val/"
#VAL_DIR = "/content/drive/MyDrive/CMPT_419_2025/EmotionData/val/"
val_dataset = EmotionSequenceDataset(VAL_DIR)
val_loader = DataLoader(val_dataset, batch_size=8)

model.eval()
y_true, y_pred = [], []

with torch.no_grad():
    for features, labels in val_loader:
        features = features.to(device)
        outputs = model(features)
        preds = torch.argmax(outputs, dim=1).cpu().numpy()
        y_true.extend(labels)
        y_pred.extend(preds)

# Evaluation Metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

print("\n Validation Results (on val/):")
print("Accuracy     :", round(accuracy_score(y_true, y_pred), 3))
print("Precision    :", round(precision_score(y_true, y_pred, average='macro'), 3))
print("Recall       :", round(recall_score(y_true, y_pred, average='macro'), 3))
print("F1 Score     :", round(f1_score(y_true, y_pred, average='macro'), 3))

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, xticklabels=EMOTIONS, yticklabels=EMOTIONS, fmt='d', cmap='Oranges')
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix - External Validation")
plt.show()

from torch.utils.data import Subset

#  Load full dataset for K-Fold use
full_dataset = EmotionSequenceDataset(TRAIN_DIR)
num_classes = len(EMOTIONS)

# KFold setup (7-fold)
kf = KFold(n_splits=7, shuffle=True, random_state=42)

#  Track final results
all_preds, all_labels = [], []
fold_metrics = []

print(" Starting 7-Fold Cross Validation")

for fold, (train_idx, val_idx) in enumerate(kf.split(full_dataset)):
    print(f"\n Fold {fold+1}/7")

    train_subset = Subset(full_dataset, train_idx)
    val_subset = Subset(full_dataset, val_idx)

    train_loader = DataLoader(train_subset, batch_size=8, shuffle=True)
    val_loader = DataLoader(val_subset, batch_size=8)

    model = EnhancedEmotionModel(num_classes=num_classes).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)

    # Train model on this fold
    for epoch in range(20):
        model.train()
        total_loss = 0
        correct = 0
        total = 0

        for features, labels in train_loader:
            features, labels = features.to(device), torch.tensor(labels).to(device)
            optimizer.zero_grad()
            outputs = model(features)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            preds = torch.argmax(outputs, dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

        scheduler.step()
        acc = correct / total
        print(f"Epoch {epoch+1:02d} | Loss: {total_loss:.4f} | Training Accuracy: {acc:.3f}")

    #  Evaluate this fold
    model.eval()
    y_true_fold, y_pred_fold = [], []
    with torch.no_grad():
        for features, labels in val_loader:
            features = features.to(device)
            outputs = model(features)
            preds = torch.argmax(outputs, dim=1).cpu().numpy()
            y_true_fold.extend(labels)
            y_pred_fold.extend(preds)

    all_labels.extend(y_true_fold)
    all_preds.extend(y_pred_fold)

    # Collect metrics for this fold
    acc = accuracy_score(y_true_fold, y_pred_fold)
    prec = precision_score(y_true_fold, y_pred_fold, average='macro')
    rec = recall_score(y_true_fold, y_pred_fold, average='macro')
    f1 = f1_score(y_true_fold, y_pred_fold, average='macro')

    fold_metrics.append({
        "Fold": fold+1,
        "Accuracy": acc,
        "Precision": prec,
        "Recall": rec,
        "F1": f1
    })

#  Summary of All Folds
print("\n Cross-Validation Metrics (Averaged over 7 folds):")
df_results = pd.DataFrame(fold_metrics)
display(df_results.round(3))
print("Mean Accuracy :", round(df_results['Accuracy'].mean(), 3))
print("Mean Precision:", round(df_results['Precision'].mean(), 3))
print("Mean Recall   :", round(df_results['Recall'].mean(), 3))
print("Mean F1 Score :", round(df_results['F1'].mean(), 3))

#  Final Confusion Matrix
cm = confusion_matrix(all_labels, all_preds)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, xticklabels=EMOTIONS, yticklabels=EMOTIONS, fmt='d', cmap='YlGnBu')
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Final Confusion Matrix (7-Fold CV)")
plt.show()





